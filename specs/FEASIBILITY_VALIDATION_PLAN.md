# Feasibility Validation Plan - PRD 1.1 Core Concepts

**Document Version:** 1.0  
**Date:** January 2026  
**Focus:** Validating AI matching, automated approvals, profile UX, and identity verification

---

## Executive Summary

Before full implementation, we must validate four critical success factors:
1. **AI Automated Matching Feasibility** - Can we reliably match crew to journeys?
2. **Automated Approval Feasibility** - Can AI safely approve applications?
3. **Profile Information Capture UX** - Can we collect data easily and accurately?
4. **Strong Identity Verification** - Can we prevent fraud while maintaining UX?

This document outlines a **Phase 0: Validation & Proof of Concept** approach.

---

## Phase 0: Feasibility Validation (Weeks 1-4)

### Objective
Prove the core concepts work with minimal implementation before committing to full development.

---

## Validation 1: AI Matching Feasibility (Week 1-2)

### 1.1 Data Collection & Analysis

**Goal:** Understand if we have enough data quality to make matching work.

**Tasks:**

1. **Audit Existing Data**
   - Review current user profiles in database
   - Assess data completeness (what fields are filled vs. empty?)
   - Identify data quality issues (inconsistent formats, missing values)
   - Document data gaps

2. **Create Sample Dataset**
   - Extract existing profiles (anonymize if needed)
   - Create synthetic profiles for testing if data is insufficient
   - Build a test set of 50-100 profiles with various skill levels/experiences
   - Build a test set of 20-30 journeys with different requirements

3. **Data Structure Validation**
   - Test data collection forms (what fields can users realistically fill?)
   - Validate skill/enum lists (are they comprehensive? not overwhelming?)
   - Check experience data formats (can we standardize this?)

**Deliverables:**
- Data quality report
- Sample dataset for matching tests
- Data structure recommendations

### 1.2 Rule-Based Matching Prototype

**Goal:** Prove matching logic works with real/simulated data.

**Tasks:**

1. **Implement Simple Scoring Algorithm** (Non-ML)
   - Skills matching (exact match, partial match, missing skills)
   - Experience level comparison
   - Preference alignment (location, duration, risk level)
   - Availability overlap
   - Weight assignment (e.g., skills 40%, experience 30%, preferences 20%, availability 10%)

2. **Test Matching Scenarios**
   - High-confidence matches (>90%)
   - Medium matches (70-90%)
   - Poor matches (<70%)
   - Edge cases (empty profiles, missing data, conflicting preferences)

3. **Generate Match Explanations**
   - Why is this a 85% match?
   - What factors contributed positively/negatively?
   - What's missing that would improve the match?

**Deliverables:**
- Matching algorithm prototype (Node.js/Python script)
- Test results: 100+ match scenarios with scores and explanations
- Match accuracy assessment (can we predict good matches?)
- Performance metrics (matching speed, scalability concerns)

**Success Criteria:**
- Can generate match scores for 90%+ of test cases
- Match explanations are clear and logical
- >70% of "high confidence" matches (>90% score) are subjectively good matches
- Algorithm runs in <1 second for 100 profiles × 50 journeys

### 1.3 User Testing - Match Quality

**Goal:** Validate that matches make sense to real users.

**Tasks:**

1. **Create Test Interface**
   - Simple UI showing match recommendations
   - Display match score, explanation, and leg details
   - Allow users to rate matches: "Good match", "Okay", "Poor match"

2. **User Testing Sessions**
   - Recruit 10-15 test users (mix of crew and owners)
   - Show them 20-30 matches generated by prototype
   - Collect feedback: Do matches make sense? Are explanations clear?
   - Measure acceptance rate: How many would they apply to?

**Deliverables:**
- User testing report
- Match quality metrics (acceptance rate, user satisfaction)
- Refinements needed for matching logic

**Success Criteria:**
- >60% of matches rated as "Good" or "Okay" by users
- Match explanations are understandable
- Users express confidence in automated matching

---

## Validation 2: Automated Approval Feasibility (Week 2-3)

### 2.1 Approval Criteria Analysis

**Goal:** Understand what owners actually care about when approving crew.

**Tasks:**

1. **Interview Owners/Captains**
   - What factors influence approval decisions?
   - What are deal-breakers? (skill gaps, experience, availability)
   - What information do they need to see?
   - How comfortable are they with automated approvals?

2. **Historical Approval Pattern Analysis** (if data exists)
   - Review past approval/rejection patterns
   - Identify common criteria
   - Find correlations (skills X + experience Y = usually approved)

**Deliverables:**
- Approval criteria matrix
- Risk factors list (what makes approvals risky?)
- Owner comfort level assessment with automation

### 2.2 Approval Scoring Prototype

**Goal:** Build and test approval scoring logic.

**Tasks:**

1. **Create Approval Scoring Algorithm**
   - Map owner requirements to crew profile
   - Check required skills/certifications
   - Verify experience thresholds
   - Calculate approval confidence score

2. **Define Approval Thresholds**
   - Auto-approve threshold (e.g., >90%)
   - Manual review threshold (e.g., 70-90%)
   - Auto-deny threshold (<70%)
   - Test thresholds with sample data

3. **Safety Checks**
   - What edge cases could cause unsafe auto-approvals?
   - Missing critical data handling
   - Conflicting information detection
   - Fraud indicators (new profiles, suspicious data)

**Deliverables:**
- Approval algorithm prototype
- Threshold recommendations
- Safety check list
- Test results: 100+ approval scenarios

**Success Criteria:**
- Algorithm can score 90%+ of test applications
- Auto-approve decisions align with manual decisions (>80% agreement)
- Safety checks catch obvious fraud/risk cases

### 2.3 Owner Feedback on Auto-Approval

**Goal:** Validate owner comfort with automated approvals.

**Tasks:**

1. **Approval Decision Preview**
   - Show owners how auto-approval would work
   - Display approval scores and reasoning
   - Simulate approval workflow

2. **Owner Interviews**
   - Would they trust auto-approval for high scores?
   - What would make them more comfortable?
   - What override scenarios do they need?
   - Feedback on approval explanations

**Deliverables:**
- Owner acceptance report
- Required features for owner confidence (override, explanations, etc.)
- Refined approval thresholds based on feedback

**Success Criteria:**
- >60% of owners comfortable with auto-approval for >90% matches
- Clear understanding of required features for trust

---

## Validation 3: Profile Information Capture UX (Week 3-4)

### 3.1 Current Profile Flow Analysis

**Goal:** Understand existing UX pain points.

**Tasks:**

1. **User Journey Mapping**
   - Map current profile creation flow
   - Identify friction points (long forms, unclear fields, data entry burden)
   - Time-to-completion measurement

2. **User Interviews**
   - What's difficult about filling profiles now?
   - What information is hard to provide?
   - What would make it easier?

**Deliverables:**
- Current UX pain points document
- User journey map
- Improvement opportunities list

### 3.2 Onboarding Wizard Prototype

**Goal:** Test new profile capture UX with real users.

**Tasks:**

1. **Design Wizard Flow**
   - Step 1: Role selection (crew vs. owner)
   - Step 2: Basic info (name, location)
   - Step 3: Skills selection (interactive, not free text)
   - Step 4: Experience input (guided questions)
   - Step 5: Preferences (visual selection)
   - Step 6: Certifications (optional upload)
   - Progress indicator, skip options, save & continue

2. **Build Clickable Prototype**
   - Use Figma/Adobe XD or simple HTML prototype
   - Focus on UX flow, not full functionality
   - Test data persistence, validation, user guidance

3. **User Testing**
   - Test with 10-15 new users
   - Measure: completion time, completion rate, user satisfaction
   - A/B test: wizard vs. single form vs. progressive disclosure

**Deliverables:**
- Onboarding wizard prototype
- User testing results (completion rates, time, satisfaction)
- UX refinements needed

**Success Criteria:**
- >80% completion rate (vs. current rate)
- Average completion time <10 minutes
- User satisfaction >4/5

### 3.3 Data Quality Validation

**Goal:** Ensure captured data is usable for AI matching.

**Tasks:**

1. **Data Completeness Testing**
   - How many users complete full profile?
   - What fields are commonly skipped?
   - Can we match with partial data?

2. **Data Accuracy Testing**
   - Validate user-entered data (skills self-assessment vs. reality)
   - Check for data inconsistencies
   - Test certification validation

**Deliverables:**
- Data quality metrics
- Required vs. optional field recommendations
- Validation rules needed

**Success Criteria:**
- Profile completion provides 70%+ of data needed for matching
- Data quality is sufficient for reliable matching

---

## Validation 4: Identity Verification & Security (Week 4)

### 4.1 Verification Service Research

**Goal:** Identify viable identity verification solutions.

**Tasks:**

1. **Service Provider Evaluation**
   - Research: Jumio, Onfido, Veriff, Persona, Stripe Identity
   - Compare: cost, features, integration complexity, coverage
   - Assess: API quality, documentation, support

2. **Feature Comparison**
   - ID document verification (passport, driver's license)
   - Selfie verification (liveness detection)
   - KYC compliance level
   - Fraud detection capabilities
   - Cost per verification
   - Success rates by region

**Deliverables:**
- Service provider comparison matrix
- Recommended provider(s)
- Cost analysis per verification

### 4.2 Integration Prototype

**Goal:** Test integration feasibility and user experience.

**Tasks:**

1. **Build Integration Proof of Concept**
   - Integrate selected provider's SDK/API
   - Test verification flow (ID upload, selfie, verification result)
   - Handle success/failure scenarios
   - Test on mobile and desktop

2. **User Experience Testing**
   - How easy is verification to complete?
   - How long does it take?
   - What are failure scenarios? (poor photo, rejected ID, network issues)
   - User feedback on verification process

3. **Security Assessment**
   - Does verification prevent obvious fraud?
   - What additional checks are needed?
   - How to handle verification failures gracefully?

**Deliverables:**
- Integration prototype
- UX test results
- Security assessment
- Recommended verification strategy

**Success Criteria:**
- Integration works reliably (90%+ success rate)
- Verification takes <5 minutes for users
- Security assessment shows fraud prevention capability

### 4.3 Fraud Prevention Strategy

**Goal:** Define how verification prevents fraud in auto-approval context.

**Tasks:**

1. **Risk Assessment**
   - What fraud scenarios are possible? (fake profiles, stolen IDs, multiple accounts)
   - How does verification mitigate each?
   - What additional safeguards are needed?

2. **Verification Requirements**
   - Who needs verification? (all users? high-value passages only?)
   - When is verification required? (on signup? before first application? before auto-approval?)
   - What happens if verification fails?

3. **Owner Confidence Testing**
   - Do owners trust auto-approval more with verification?
   - What verification info should owners see?

**Deliverables:**
- Fraud prevention strategy document
- Verification requirements matrix
- Owner trust assessment

**Success Criteria:**
- Clear fraud prevention strategy
- Verification increases owner confidence in auto-approval (>50% improvement)

---

## Validation Summary & Go/No-Go Decision

### Week 4 Deliverable: Feasibility Report

**Contents:**

1. **AI Matching Validation**
   - Matching algorithm works: ✅/❌
   - Match quality acceptable: ✅/❌
   - Users trust matches: ✅/❌

2. **Automated Approval Validation**
   - Approval logic works: ✅/❌
   - Safety checks sufficient: ✅/❌
   - Owners trust auto-approval: ✅/❌

3. **Profile UX Validation**
   - Wizard improves completion: ✅/❌
   - Data quality sufficient: ✅/❌
   - Users satisfied: ✅/❌

4. **Identity Verification Validation**
   - Integration feasible: ✅/❌
   - UX acceptable: ✅/❌
   - Fraud prevention effective: ✅/❌

5. **Recommendations**
   - Proceed with full implementation?
   - What needs refinement before proceeding?
   - What risks/concerns remain?

### Go/No-Go Criteria

**Proceed if:**
- ✅ Matching algorithm generates quality matches (>60% user acceptance)
- ✅ Approval logic is safe and trusted (>60% owner acceptance)
- ✅ Profile UX improves completion significantly (>80% completion rate)
- ✅ Identity verification is feasible and effective

**Refine/Revise if:**
- ⚠️ One or more validations show concerns but are fixable
- ⚠️ Data quality needs improvement before matching works

**Reconsider if:**
- ❌ Core concepts don't work (matching fails, approval unsafe)
- ❌ User resistance is too high (owners don't trust automation)
- ❌ Technical blockers are insurmountable

---

## Recommended First Steps (This Week)

### Immediate Actions:

1. **Day 1-2: Data Audit**
   - Review existing database
   - Assess data quality/completeness
   - Identify gaps

2. **Day 3-4: Build Simple Matching Prototype**
   - Create rule-based matching algorithm
   - Test with sample data (10 profiles, 5 journeys)
   - Generate match scores and explanations

3. **Day 5: Owner/Crew Interviews**
   - Interview 5 owners about approval criteria
   - Interview 5 crew about profile UX pain points
   - Validate assumptions about what matters

4. **Week 2: Continue with full validation plan**

### Quick Wins to Validate Early:

1. **Test Matching Concept**
   - Manual matching exercise: Take 5 crew profiles, 3 journeys
   - Manually score matches using proposed algorithm logic
   - Validate: Do high scores = good matches?

2. **Sketch Onboarding Wizard**
   - Draw wireframes of wizard flow
   - Test with 2-3 users (paper prototype)
   - Validate: Is this easier than current form?

3. **Research Verification Services**
   - Request demos from Jumio/Onfido
   - Get pricing quotes
   - Assess integration complexity

---

## Risk Mitigation

### High-Risk Areas:

1. **Matching Quality**
   - Risk: AI matching doesn't produce good matches
   - Mitigation: Start with rule-based, test extensively, iterate before ML

2. **Owner Trust**
   - Risk: Owners don't trust automated approvals
   - Mitigation: Extensive owner interviews, transparency, override options

3. **Identity Verification UX**
   - Risk: Verification is too cumbersome, users drop off
   - Mitigation: Test UX early, optimize flow, make it optional initially

4. **Data Quality**
   - Risk: Insufficient data for good matching
   - Mitigation: Improve onboarding UX, incentivize completion, validate early

---

## Next Steps After Validation

If validation is successful:
- Proceed to Phase 1 of Implementation Plan (Foundation & Data Infrastructure)
- Use validation learnings to refine implementation approach
- Address any concerns identified during validation

If validation shows concerns:
- Refine concepts based on feedback
- Re-run validation for specific areas
- Consider alternative approaches

---

## Questions to Answer During Validation

1. **Matching:**
   - Can we reliably match crew to journeys with >70% accuracy?
   - Are match explanations clear enough?

2. **Approval:**
   - What approval score threshold gives acceptable accuracy?
   - Do owners trust automation at that threshold?

3. **Profile UX:**
   - Can we collect enough data without overwhelming users?
   - How can we improve profile completion rates?

4. **Identity Verification:**
   - Which verification provider gives best UX/security balance?
   - How do we handle verification failures gracefully?

5. **Overall:**
   - Are users ready for automation?
   - What's the minimum viable automation that users will accept?
